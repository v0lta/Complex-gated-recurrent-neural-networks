\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage[utf8]{inputenc}
\usepackage{fontspec} % This line only for XeLaTeX and LuaLaTeX
\usepackage{pgfplots}
\usepgfplotslibrary{groupplots}
\pgfplotsset{compat=1.13}
\usepackage[hidelinks]{hyperref}
\usepackage{standalone}
\include{draft_AY}

\title{Complex unitary memory units}
\author{Moritz Wolter  \\
    Uni Bonn  \\
    \and 
    Angela Yao \\
    Uni Bonn \\
    }

\date{\today}
% Hint: \title{what ever}, \author{who care} and \date{when ever} could stand 
% before or after the \begin{document} command 
% BUT the \maketitle command MUST come AFTER the \begin{document} command! 
\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
RNN optimization often suffers from numberically unstable gradients. We propose a novel complex RNN architecture, which can be shown to be numerically stable. Building on top of recent sucesses in the optimization of complex valued neural networks we propose a novel memory cell, which allows us to take gated recurrent units to the complex domain. In short we optimize: 
\begin{align}
\min_{\mathbf{W}} \text{cost}(\{\mathbf{x}\}, \{\mathbf{W}\}) \\
\text{such that } \forall m \; \| \phi_m \| = 1, \\
                  \forall n \; \| f'(h_n) \| \leq 1.
\end{align}
Where $\{\mathbf{W}\}$ denotes the set of network weights $\{\mathbf{x}\}$ the set of network inputs and $\{\phi_m\}$ the set of all weight matrix eigenvalues and finally $\|f'(h_n) \|$ the hidden actication derivatives.
We show that our complex gated memory cells are practically stable and use wirtinger calculus to overcome limitations on skalar activations set by Liouville's theorem. It turns out that we do not need to work with a constrained optimization algorithm here, but can instead rewrite the problem in an unconstrained way, and use libraries optimized for large scale unconstrained optimization such as tensorflow or pytorch.
\end{abstract}

\section{Introduction}
Recurrent neural networks (RNNs) are widely used for processing time series and sequential information.  The difficulties of training RNNs, especially when trying to learn long-term dependencies, are well-established, as RNNs are prone to vanishing and exploding gradients~\cite{bengio1994learning,Hochreiter,Pascanu}.  
% The training process of artificial neural networks is not necessarily stable. Unstable 
% ~\MW{problem formulations lead to problems which are hard to optimize and converge slowly}.
Heuristics have been developed to alleviate some of the optimization instabilities and learning difficulties.  They include gradient clipping~\AY{ref?}, gating, as used in gated recurrent units (GRUs) and long short-term memory (LSTM) networks, and using norm-preserving weight matrices. RNNs of the latter type are particularly interesting because they are guaranteed to be mathematically stable~\cite{Arjovsky}. To be norm-preserving, weight matrices need to be either orthogonal or unitary\footnote{Unitary matrices are the complex analogue of orthogonal matrices, \ie a complex matrix $W$ is unitary if $W W^* = W^* W = I$, where $W^*$ is its conjugate transpose and $I$ is the identity matrix.~\AY{check necessity of square in definition}.}.  

Arjovsky~\etal~\cite{Arjovsky}, as well as follow-up works~\cite{Hyland, Wisdom} advocate working in the complex domain, since enforcing weight matrices to be unitary is less restrictive than enforcing orthogonality. Unitary Matrices can spread their eigenvalues over the entire unit circle, while orthogonal matrices have eigenvalues of either minus one or one.  However, complex neural networks have been receiving attention in their own right~\AY{add citations of complex networks}. ~\AY{add reasons why we want to work in the complex domain; see benefits form chapter 2 of Mandic, complex filtering book}. ~\AY{It is our interest to develop the mathematical theory for working with complex recurrent neural networks}.  

We (along with many others~\AY{citations}) posit that working in the complex domain can significantly increase the functionality of neural networks while simplifying the learning task.  Complex neural networks are more than simply a real-valued counterpart with twice as many parameters / dimensions.

Directly extending the mathematics of RNNs to complex networks is non-trivial because of~\AY{x and Y}. ~\AY{To date, has only been shown for CNNs, no gates, etc.} In this work, we propose a stable gated RNN~\AY{discuss contributions of paper}.


\section{Related work}
Normalized complex matrices where first introduced into the literature by \cite{Arjovsky}. Since then \cite{Wisdom}, expanded the reach of the unitary matrix basis. An idea that is taken further by \cite{Hyland}, which makes use Lie group theory.  
A holomorph non-linearity was used in \cite{Guberman}, \cite{Arjovsky} introduces a novel non-linearity which is not complex-differentiable. \cite{Trabelsi} compares complex non-linearities and systematically measures performance. Furthermore complex batch-normalization is introduced.
Finally \cite{Jing}, proposes a gated unitary RNN, but is restricted to the real numbers. \\
Finding a gradient for functions from $\mathbb{C}$ to $\mathbb{R}$, is a problem which has been adressed in the digital signal processing literature. Where complex problems with real cost functions had to be solved \cite{Brandwood}\cite{Bos}\cite{Franken}\cite{Delgado}. Applications to neural network cost functions where considered later \cite{Mandic}. All solutions essentially utilize Wirtinger calculus \cite{Wirtinger}, to come up with an approximate gradient for a non-holomorph function.

\section{Preliminaries}
A complex-valued function $f$ mapping 

\AY{add small primer on requirements for complex networks, holomorph, Wirtinger calculus / pseudo-gradients; see section II of Scardapane section II}

\subsection{$\mathbb{C}\mathbb{R}$ Calculus}
\AY{definition of z = u + iv; separation of complex into real and imaginary}\\
\AY{to be truly differentiable / holomorph, needs to satisfy Cauchy-Riemann conditions; most people are put off by Liouville's condition.}
\AY{however, can leverage the use of CR-Calculus, also known as Wirtinger calulus.  In such a scenario, add equations of real and imaginary derivative; approximations}


\subsection{Complex Activation Functions}
\begin{itemize}
    \item non-linear and bounded
    \item for learning, partial derivatives should exist and also be bounded
\end{itemize}

Based on Liousville's theorem, functions from $\mathbb{C} \rightarrow \mathbb{R}$ cannot be holomorph unless they are constant


\section{Complex Gated Network}
\subsection{Norm-Preserving RNNs}
Suppose we are given a neural network with $T$ hidden layers and an objective function $C$.  Let $x_t$ and $h_t$ represent the input and hidden unit vectors at layer $t$, $f$ be a point-wise non-linearity function, and $W_t$ and $V_t$ be the hidden and input weight matrices respectively.  The network can be defined as
\begin{align}
    % h_{t+1} &= f(\bW_t h_t + \bV_t x_{t+1})
    z_{t+1} &= \bW_t h_t + \bV_t x_{t+1} \\
    h_{t+1} &= f(z_{t+1}).
\end{align}

A matrix $\bW$ is norm-preserving if its repeated multiplication with a vector leaves the vector norm unchanged, \ie $\Vert \bW h\Vert_{2} = \Vert h \Vert_2$.  If $\bW$ contains only real-valued entries, and $\bW^\intercal \bW =  \bW \bW^\intercal = \bm{I}$.  If $\bW$ contains entries from the complex domain, it is \emph{unitary} and $\bW^* \bW = \bW \bW^* = \bm{I}$, where $\bW^*$ is the complex conjugate transpose of $\bW$.  

In~\cite{Arjovsky}, Arjovsky~\etal~prove that with an orthogonal or unitary weight matrices $\bW_k$, one can avoid exploding gradients of the cost function $C$ with respect to $h_t$.  More specifically, the gradient magnitude can be bounded as follows

\begin{align}\label{eq:gradbound}
        \left\Vert\frac{\partial C}{\partial h_t}\right\Vert & \leq   \left\Vert\frac{\partial C}{\partial h_T}\right\Vert \prod_{k=t}^{T-1}\Vert \bD_{k+1} \bW _k^T \Vert 
       =  \left\Vert\frac{\partial C}{\partial h_T}\right\Vert \prod_{k=t}^{T-1}\Vert \bD_{k+1} \Vert  =  \left\Vert\frac{\partial C}{\partial h_T}\right\Vert,
\end{align}

where $\bD_k = \text{diag}(f'(z_{k}))$ is the Jacobian matrix of the pointwise non-linearity.

Since $\bD_k$ is a diagonal matrix, $\Vert\bD_k\Vert = \max_{j=1,...,n} |f'(z_k^{(j)})|$, where $z_k^{(j)}$ is the $j^{\text{th}}$ pre-activation of the $k^{\text{th}}$ hidden layer.  This proof hinges the critical assumption that $\Vert\bD_k\Vert = 1$ for all layers $k$.  For real pre-activations, \ie $z \in \mathbb{R}$, this constraint is easily met with the standard rectified linear unit or ReLU.  For complex pre-activations $z \in \mathbb{C}$, however, one needs a non-linearity that is applicable to complex inputs. More importantly, 

\subsection{Complex Activation Functions}
The \emph{modReLU} proposed by Arjovsky \etal in~\cite{Arjovsky}, defined as 
\begin{equation}
\sigma_{\text{modReLU}}(z) = \text{Relu}(\|z\| + b) \frac{z}{\|z\|}.
\end{equation}

As noted by Arjovsky, this \emph{is} the case for the standard rectified linear unit or ReLU.  However, the standard ReLU is defined only if the pre-activation is real \ie $z_k^{(j)} \in \mathbb{R}$.  for a complex pre-activation, one needs a specially defined non-linearity applicable to complex inputs.  Arjovsky define a modRelu, however, 
.  With a complex pre-activation $z_k^{(j)} \in \mathbb{C}$, it is complex\footnote{the only case it can be real with complex $\bW$ is if $h_t$ is the complex conjugate.}, as is proposed 
$\sigma$ must be a bounded holomorph function 
However, this
due to liouville theroem, cannot be both analytic and bounded; resort to using an analytic version (always differentiable), i.e. complex tanh / complex sigmoid, then we end up with singularities and lose on the boundedness; may be difficult to avoid whilst learning;
alternative, settle for approximations without being analytic i.e. go for partial derivatives not defined everywhere; stay for bounded
modReLU is neither analytic nor bounded but if we stay mostly in the linear range, then won't have problems (check values in arjovsky's experiment results to verify)

\subsubsection{Stiefel-manifold optimization}
The group of unitary/orthogonal matrices is not closed under addition. Using orthogonal initialization and standard addition based gradient descent leads to loss orthogonality. In order to overcome this problem Wisdom et al.\cite{Wisdom} uses the Stiefel-Manifold optimization scheme described in\cite{Tagare}. Tagare proposes to use a gradient, which points along the Stiefel-Manifold of unitary matrices, which
itself unitary. This gradient may be computed using \cite{Tagare}:
\begin{align}
\mathbf{A} &= \mathbf{W}\mathbf{G}^H - \mathbf{W}^H\mathbf{G} \\
Y^k(\lambda) &=  (\mathbf{I} + \frac{\lambda}{2}\mathbf{A}^k)^{-1}(\mathbf{I} - \frac{\lambda}{2}\mathbf{A}^k)\mathbf{W}^k
\end{align}
With the resulting weight update given by $\mathbf{W}^{k+1} = Y^k(\lambda)$. 

\subsection{Complex Gating}
desirable properties for complex gate
(1) we want a $\mathbb{C} \to \mathbb{R}$ mapping for the gates because we want to preserve (not modify) the phase (cite Arjovsky, also show experimentation, showing those which change the phase and those which do not)
(2) have a non-linearity, so as to get a bounded gate value (from 0 to 1); non-linearity is good for learning approximation --> which non-linearity is good? show different baselines
(3) including the gates should still conserve the overall stability of the architecture (distribution over a sum is a no-no --> LSTM distributes over a sum; compare with baseline which the gates are not constrained in any way) 


\section{Complex gated recurrent units}

\subsection{The gated unitary evolution network}
\begin{align}
\mathbf{i}_g &= \sigma(\mathbf{W}_i[\Re(\mathbf{x}) \; \Im(\mathbf{x})]^T),  \\
\mathbf{f}_g &= \sigma(\mathbf{W}_f[\Re(\mathbf{x}) \; \Im(\mathbf{x})]^T),  \\
\mathbf{h}_{t+1} &= \mathbf{U}_h f(\mathbf{f}_g \odot \mathbf{h}_t) + \mathbf{W}_x(\mathbf{i}_g \odot \mathbf{x}).
\end{align}
For notational brevity bias terms are omitted. $\mathbf{U}_h$ denotes a unitary matrix and $\mathbf{i}_g, \mathbf{f}_g$ are computed by mappings from $\mathbb{C} \rightarrow \mathbb{R}$. Our gates are therefore non-holomorph. We leverage Wirtinger calculus \cite{Franken}\cite{Delgado}\cite{Wirtinger}, to define a pseudogradient, which we argue is sufficient to train the gates. Because $\mathbf{U}_h$ is unitary as in \cite{Arjovsky}, we do not have to distribute our derivatives over a sum, a major difference with respect to the classical formulation in \cite{Hochreiter}. Because gate output is independent of the cell state $\mathbf{h}$, the proof in \cite{Arjovsky} holds without modification since $\sigma(\cdot) \in [0, 1]$.

\subsection{The complex gated recurrent unit}
Setting up complex gating mechanisms is no trivial task, because functions from $\mathbb{C} \rightarrow \mathbb{R}$ cannot be holomorph unless they are constant \cite[page 9]{Bornemann}\footnote{Proof: \url{https://math.stackexchange.com/questions/1004672/prove-that-a-real-valued-constant-function-is-holomorphic-and-vice-versa}}. Furthermore bounded holomorph complex functions must be constant \cite[page 38]{Bornemann}\footnote{\url{https://en.wikipedia.org/wiki/Liouville\%27s\_theorem\_(complex\_analysis)}}.
Classic multiplication gates with $0 \leq |f(x)| \leq 1$ and $\mathbb{C} \rightarrow \mathbb{R}$ which rely on $f(x) \cdot h$ are therefore hard to implement, because there is no obvious complex gradient to train these gates. 

In order to take the classic GRU to the complex domain, while inheriting as many of the favorable properties from \cite{Arjovsky} as possible we define:
\begin{align}
\mathbf{g} &= \mathbf{O}_g \mathbf{h} + \mathbf{W}_g\mathbf{x} + \mathbf{b}_g, \\
\mathbf{z} &= \sigma(\Re(\mathbf{g})), \\
\mathbf{r} &= \sigma(\Im(\mathbf{g})), \\
\overline{\mathbf{h}}_{t} &= f(\mathbf{W}_x \mathbf{x}_t + \mathbf{U}_h (\mathbf{r} \odot \mathbf{h}_{t-1}) + \mathbf{b}_h) \\
\mathbf{h} &= (1 - \mathbf{z}_t) \odot \mathbf{h}_{t-1} +  \mathbf{z}_t \odot \overline{\mathbf{h}_t} \label{eq:gru-sum}
\end{align}
With $\mathbf{U}_h$ defined as a unitary matrix. And $\mathbf{O}$ defined as split orthogonal, which means we define $\Re(\mathbf{O})^{T}\Re(\mathbf{O}) = \mathbf{I} = \Im(\mathbf{O})^{T}\Im(\mathbf{O})$, a definition we motivate in the upcoming section. $f$ denotes the non-linearity. Please note that since $\mathbf{r}$ and $\mathbf{f}$ are real vectors. Multpilication with these vectors therefore changes only the magnitude, but leaves the phase unchanged. A major difference with respect to real valued memory cells. This definition allows the model to scale data points according to their current relevance, while leaving phase information untouched. \\
For $\mathbf{z}, \mathbf{r} = \mathbf{1}$ the equations above simplify to the formulation proposed in $\cite{Arjovsky}$. Previous work has shown that initializing memory gates to be open is beneficial \MW{TODO: Find citations}.

\subsection{Model stability}
For $\mathbf{z} \neq \mathbf{1}$ derivatives will distribute over a sum, forming a constant error carrousel as described in \cite{Hochreiter}. Choosing the gate bias to be large we initially have $\mathbf{z} \approx \mathbf{1}$ and a situation similar to \cite{Arjovsky} considering their formulation, while taking into account the added reset gate we obtain: 
\begin{align}
\frac{\partial C}{\partial \mathbf{h}_{t}} &= \frac{\partial C}{\partial \mathbf{h}_T}\frac{\partial 
                                              \mathbf{h}_T}{\partial \mathbf{h}_t}, 
                                     \label{eq:peep_cost} \\
                                  &= \frac{\partial C}{\partial \mathbf{h}_T}\prod_{k=t}^{T-1}\frac{\partial \mathbf{h}_{k+1}}{\partial \mathbf{h}_k}, \\
                                  &= \frac{\partial C}{\partial \mathbf{h}_T}\prod_{k=t}^{T-1}\mathbf{U}_h
                                  \mathbf{G}_{k+1} \mathbf{R}_{k+1}
\end{align}
With $\mathbf{U}$ unitary and $\mathbf{G} = \text{diag} (f'(\mathbf{r} \odot \mathbf{h}_t))$. The matrix $\mathbf{R}$ and $\mathbf{Z}$ represent the change introduced by the gates. The change in gradient dynamics introduced by the reset gate is therefore governed by $\partial \mathbf{r}/ \partial \mathbf{h}_k$ as well as $\partial \mathbf{z}/ \partial \mathbf{h}_k$. 
Wirtinger-calculus tells us that \cite{Remmert}[page 55],\cite{Mandic}[page 61, eq 5.14]:
\begin{align}
\frac{\partial \mathbf{r}}{\partial \mathbf{h}_k} &= \frac{\partial \mathbf{r}}{\partial \mathbf{g}_k}\frac{\partial \mathbf{g}_k}{\partial \mathbf{h}_k} + \frac{\partial \mathbf{r}}{\partial \overline{\mathbf{g}_k}}\frac{\partial \overline{\mathbf{g}_k}}{\partial \mathbf{h}_k}  & \text{Wirtinger chain rule}\\
                                &= \frac{1}{2i}\text{diag}(\sigma'(\frac{\mathbf{g} - \overline{\mathbf{g}}}{2i}))\mathbf{O}_g - \frac{1}{2i}\text{diag}(\sigma'(\frac{\mathbf{g} - \overline{\mathbf{g}}}{2i}))\overline{\mathbf{O}}_g \\
                                &= \mathbf{R}(\frac{\mathbf{O}_g - \overline{\mathbf{U}}_g}{2i})  & \mathbf{R} = \text{diag}(\sigma'(\frac{\mathbf{g} - \overline{\mathbf{g}}}{2i}))\\
                                &= \mathbf{R}\Im(\mathbf{O}_g) \\
\frac{\partial \mathbf{z}}{\partial \mathbf{h}_k} &= \frac{\partial \mathbf{z}}{\partial \mathbf{g}_k}\frac{\partial \mathbf{g}_k}{\partial \mathbf{h}_k} + \frac{\partial \mathbf{z}}{\partial \overline{\mathbf{g}_k}}\frac{\partial \overline{\mathbf{g}_k}}{\partial \mathbf{h}_k}  & \\
                                &= \frac{1}{2}\text{diag}(\sigma'(\frac{\mathbf{g} + \overline{\mathbf{g}}}{2}))\mathbf{O}_g +   \frac{1}{2}\text{diag}(\sigma'(\frac{\mathbf{g} + \overline{\mathbf{g}}}{2}))\overline{\mathbf{O}}_g \\
                                &= \mathbf{Z}(\frac{\mathbf{O}_g + \overline{\mathbf{U}}_g}{2}) & \mathbf{Z} = \text{diag}(\sigma'(\frac{\mathbf{g} + \overline{\mathbf{g}}}{2})) \\
                                &= \mathbf{Z}\Re(\mathbf{O}_g) \\
\end{align}
Taking the norm we obtain:
\begin{align}
|\frac{\partial \mathbf{r}}{\partial \mathbf{h}_k}| &= | \mathbf{R}| \; |\Im(\mathbf{O}_g)| \\
|\frac{\partial \mathbf{z}}{\partial \mathbf{h}_k}| &= | \mathbf{Z} \; |\Re(\mathbf{O}_g)|
\end{align}
Constraining $\mathbf{O}_g$ to have orthogonal real and imaginary part, $|\Im(\mathbf{O}_g)| = |\Re(\mathbf{O}_g)| = 1$ and going back to the cost function term ~\ref{eq:peep_cost}, considering it's norm leads to:
\begin{align}
|\frac{\partial C}{\partial \mathbf{h}_t}| = |\frac{\partial C}{\partial \mathbf{h}_T}| \:  \prod_{k=t}^{T-1} 
                                  |\mathbf{G}_{k+1}| \cdot |\mathbf{R}_{k+1}|.
\end{align}
With $\mathbf{G} = \text{diag} (f'(\mathbf{r} \odot \mathbf{h}_t))$ and $\mathbf{R} = \text{diag}(\sigma'(\frac{\mathbf{g} + \overline{\mathbf{g}}}{2}))$. We desire $|\frac{\partial C}{\partial h_t}| = |\frac{\partial C}{\partial h_T}|$, which would hold only if we could guarantee $|\mathbf{G}_{k+1} \mathbf{R}_{k+1}| = 1$. $\mathbf{R}$ is populated with values drawn from a bell-shaped curve, we therefore expect $|\mathbf{R}| \leq 1$ \MW{The experiments with the gate-relu failed, because gate gradients vanish after a while. Here I am talking about sigmoid(4x)}. To prevent gradients from vanishing for longer time sequences, equation~\ref{eq:gru-sum} introduces a constant error carousel as described in \cite{Hochreiter}. We argue that our hybrid approach inherits some of the capabilities of unitary evolution networks by bounding the update in equation in~\ref{eq:gru-sum}, while at the same time gaining the noise resistance that comes with gated memory management.\\


\subsection{Results}
\begin{figure}
\centering
%\includestandalone[width=0.45\linewidth]{./img/cgmu/adding_problem}
\includegraphics[width=0.45\linewidth]{./img/cgmu/adding_problem.pdf}
%\includestandalone[width=0.45\linewidth]{./img/cgmu/memory_problem}
\includegraphics[width=0.45\linewidth]{./img/cgmu/memory_problem.pdf}
\caption{Performance of the complex gated memory unit (cgmu, ours), the unitary neural network (unn, \cite{Arjovsky}), and long short term memory (lstm, \cite{Hochreiter}), T=100 unitary U, free O, no GRU equations.}
\label{fig:montreal_eval}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=0.45\linewidth]{./img/cgmu/adding_problem_250_lstm_cgru_hirose_mrelu.pdf}
\includegraphics[width=0.45\linewidth]{./img/cgmu/memory_problem_250_lstm_cgru_hirose_mrelu.pdf}
\caption{T=250, unitary U, free O, GRU, nlstm=128, ncgru=48}
\label{fig:montreal_eval2}
\centering
\includegraphics[width=0.45\linewidth]{./img/cgmu/adding_problem_100_norm_no_norm_hirose.pdf}
\includegraphics[width=0.45\linewidth]{./img/cgmu/memory_problem_100_norm_no_norm_mod_relu.pdf}
\caption{T=100, orthogonal unitary comparison, mod relu for memory, hirose for adding, GRU equation. nlstm=128, ncgru=48}
\label{fig:montreal_eval3}
\centering
\includegraphics[width=0.45\linewidth]{./img/cgmu/adding_problem_250_norm_no_norm_hirose.pdf}
\includegraphics[width=0.45\linewidth]{./img/cgmu/memory_problem_250_norm_no_norm_mod_relu.pdf}
\caption{T=250, orthogonal unitary comparison, mod relu for memory, hirose for adding, GRU equation. nlstm=128, ncgru=48}
\label{fig:montreal_eval3}
\centering
\end{figure}
\begin{figure}
\includegraphics[width=0.45\linewidth]{./img/cgmu/adding_problem_250_wisdom.pdf}
\includegraphics[width=0.45\linewidth]{./img/cgmu/memory_problem_250_wisdom.pdf}
\caption{T=250, unitary evolution RNN proposed by wisdom et al. vs. LSTM nlstm=128, nUNN=128}
\label{fig:montreal_eval3}
\end{figure}

To test our ideas we used the benchmark originally proposed in \cite{Hochreiter} following the implementation of \cite{Arjovsky}. A visualization of our results is shown in figure~\ref{fig:montreal_eval}. All modes where run with a state size of $512$, step size of $0.001$ and a batch size of $250$. For the memory problem, the baseline is at $0.173$, which all models beat. For the adding problem it is $0.167$. Again all models crack this treshhold. However the convergence behaviour differes. We argue that our apprach gets the bost of both worlds in terms of performance an converges well on both the adding and memory problems, it shows the supereor UNN dynamics on the memory problem, while at the same time behaving more like the LSTM on the adding problem, where it significantly outperforms both other models.


\subsection{Computational Expense}
Experimental baseline to show why complex networks are better than their counterpart with same number of params

\section{Conclusion}
TODO.



\section{Other Ideas}
\subsection{Fourier Space rotations.}
Earlier work has found that rotations can be implemented in the frequency domain by shearing along 
the two dimensions. Making use of the DFTs shift theorem \cite[page 173]{Briggs}:\footnote{\url{http://www.nontrivialzeros.net/KGL_Papers/27_Rotation_Paper_1997_qualityscan_OCR.pdf}}
\footnote{\url{http://bigwww.epfl.ch/publications/unser9502.pdf}}
\begin{align}
\mathcal{D}(f_{m + m_0, n + n_0}) = \omega_M^{-m_0j}\omega_N^{-n_0k}F_{jk} \\
\text{with } \omega_N^{nk} = e^{i2\pi nk/N}
\end{align}
Transformation to the frequency domain multiplication, rotation and inverse transformation, can be implemented using three matrix multiplications, when working with the DFT or as FFT, multiplication and ifft. The inverse transformation is a way to implicitly apply trigonometric interpolation\footnote{\url{https://en.wikipedia.org/wiki/Trigonometric_interpolation}}. Which takes care of interpolating the pixel values of the new rotated image.

Some first numerical evidence suggests that fourier rotation matrices are unitary. This could allow us to prove stability. TODO: Proof?

\subsection{Unitary dynamic filter networks}
Motivation: Current dynamic RNNs do not worry about stability. \\
Idea: Adapt RNN stability theory to come up with stable dynamic RNNs. \\
Extra motivation: I think that the steerable filter paper \cite{Freeman} was the basis for the original dynamic filter paper. Which is why I think the fourier extension of this paper \cite{Michaelis} could hold some cues for a nice extension. In particular, because outside of the vision domain, \cite{Hyland} has already shown that this is an interesting idea. 

\section{Background}

\subsection{Phase-Relus}
Holomorph functions $f(x,y) = u(x,y) + iv(x,y)$ must satisfy the Cauchy-Riemann equations:
\begin{align}
\frac{\partial u}{\partial x} = \frac{\partial u}{\partial y} \; \text{and} \; \frac{\partial u}{\partial y} = -\frac{\partial v}{\partial x}  
\end{align}
This form has been considered in \cite{Trabelsi} and used to evalute existing non-linearities such as the zRelu, cRelu or Mod-Relu. However we believe it is much more intuitive to consider the Cauchy-Riemann equations in polar form \footnote{Proof see: \url{https://math.stackexchange.com/
questions/1245754/cauchy-riemann-equations-in-polar-form}}:
\begin{align}
\frac{\partial u}{\partial r} = \frac{1}{r} \frac{\partial v}{\partial \theta} \; \text{and} \; \frac{\partial v}{\partial r} = - \frac{1}{r} \frac{\partial u}{\partial \theta}
\end{align}
Which allows us to design a non-linearity using $z = re^{i\theta}$ and $f(r,\theta ) = u(r,\theta ) + iv(r,\theta )$. We will focus on non-linearities of the form:
\begin{align}
f(r, \theta) &= g(r,\theta,a,b) e^{i \theta} \\
&= g(r,\theta,a,b)\cos(\theta) + ig(r,\theta,a,b)\sin(\theta)
\end{align}
With $a,b$ as lernable function parameters. Setting $g(r,\theta,a,b)$ to:
\begin{align}
g(r,\theta,a,b) = r \text{H}(\sin (\theta \cdot a\pi + b))
\label{eq:PhaseRelu}
\end{align}
With H denoting the Heaviside step function and $a,b \in \mathbb{R}$.
\begin{figure}
    \centering
    \includestandalone{image-code/sine_step}
    \includestandalone{image-code/shifted_sine_step}
    \caption{Plot of the $\sin(\theta a\pi + b\pi)$ and $\text{H}(\sin(\theta a\pi + b\pi))$ with $a=1, b=0$ (left) and $a=2, b=0.1$}
    \label{fig:step_sine}
\end{figure}
Leads to the conditions:
\begin{align}
\frac{\partial u}{\partial r} &= \text{H}(\sin (\theta \cdot a\pi + b)) \cos \theta, \\ 
\frac{\partial v}{\partial r} &= \text{H}(\sin (\theta \cdot a\pi + b)) \sin \theta, \\
\frac{\partial u}{\partial \theta} &= -r \text{H}(\sin (\theta \cdot a\pi + b)) \sin \theta \nonumber \\
&\quad + r \delta(\sin (\theta \cdot a\pi + b)) \cos(\theta \cdot a\pi + b))a\pi \cos(\theta)\\ 
\frac{\partial v}{\partial \theta} &= r \text{H}(\sin (\theta \cdot a\pi + b)) \cos \theta \nonumber \\
&\quad + r \delta(\sin (\theta \cdot a\pi + b)) \cos(\theta \cdot a\pi + b))a\pi \sin(\theta)
\end{align}
Above $\delta$ denotes Dirac's distribution, which we consider to be zero for all practical purposes. 
We therefore argue that this non-linearity which we call Polar-Relu is approximately holomorph\footnote{Strictly speaking it is holomorph, when excluding all points where $\sin (\theta \cdot a\pi + b)$ = 0.}.\\
$\text{H}(\sin(\theta \cdot a \pi + b)$ sets the output to zero, whenever $\sin (\theta \cdot a\pi + b) < 0$. We must have $\theta \in [0, 2\pi]$. This means for $a = 1, b = 0$ this non-linearity removes the lower-half of the complex plane with $\theta > \pi$ where $\Re(z) < 0$. When keeping $b=0$, for $0.5 < |a| < 1$ the filtered spectrum is reduced, and for $|a| < 0.5$, no values are filtered. Working with $|a| > 1$ introduces periodically spaced smaller filters. Because the sine wave will complete more than one iteration for $\theta$. Finally $b$ rotates the filter around the origin, this parameter enables layered phase relus to individually remove different areas of the complex plane. \\
An interesting variant of this approach can be created by adding a cosine term to equation~\ref{eq:PhaseRelu}:
\begin{align}
g(r,\theta,a,b,c,d) = r \text{H}(\sin (\theta \cdot a\pi + b))\text{H}(\cos (\theta \cdot c\pi + d))
\end{align}
This will kill any incoming complex number with a phase angle of either zero sine or cosine. The above equation can be considered a generalization of the zRelu from \cite{Guberman}\cite{Trabelsi}. Its is equivalent for $a=1, b=0, c=1, d=0$ because both cosine and sine are positive in the first quadrant. This approach works when choosing the function parameters manually. Unfortunately, the same mechanics, that makes this approach approximately holomorph also kills the derivative, which one would want to use to train the function parameters.  \\

\subsubsection{Phase-Relu approximate stability proof in Cartesian coordinates.}
\label{sec:CarthesianPhaseRelu}
We have shown that 
\begin{align}
f(r,\theta) = r\text{H}\sin(\theta \cdot a \pi + b)e^{i\theta}
\end{align}
is stable in polar coordinates. For the extremely skeptical reader we will now show that its equivalent form:
\begin{align}
f(x + iy) = \text{H}(\sin(\text{atan2}(x,y)))(x + iy)
\end{align}
is stable in Cartesian coordinates.
Splitting the above formulation into real and imaginary parts leads to:
\begin{align}
f(x + iy) = x\text{H}(\sin(\text{atan2}(x,y))) + i y\text{H}(\sin(\text{atan2}(y/x)))
\end{align} 
We recognize the form $f(z) = u + iv$. Working with the unchanged Cauchy-Riemann equations:
\begin{align}
\frac{\partial u}{\partial x} = \frac{\partial v}{\partial y}, \;\;\; \frac{\partial u}{\partial y} = - \frac{\partial v}{\partial x},
\end{align}
and using the facts that the derivatives of $\text{atan2}(x,y)$ are equal to those of $\tan^{-1}(y/x)$ which are $\frac{\partial \tan^{-1}(y/x)}{\partial x} = -\frac{y}{x^2 + y^2}$ and $\frac{\partial \tan^{-1}(y/x)}{\partial y} = \frac{x}{x^2 + y^2}$, we derive:
\begin{align}
\frac{\partial u}{\partial x} &= \text{H}(\sin(\tan^{-1}(y/x))) \nonumber \\
&\quad +x\delta(\sin(\text{tan}^{-1}(y/x)))\cos(\tan^{-1}(y/x))(\frac{-y}{x^2 + y^2})(\frac{-y}{x^2}) \\
\frac{\partial u}{\partial y} &= \delta(\sin(\tan^{-1}( y/x)))\cos(\tan^{-1}(y/x))(\frac{x}{x^2 + y^2}) \\
\frac{\partial v}{\partial x} &= y\delta(\sin(\tan^{-1}( y/x)))\cos(\tan^{-1}(y/x))(\frac{-y}{x^2 + y^2})(\frac{-y}{x^2}) \\
\frac{\partial v}{\partial y} &= \text{H}(\sin(\tan^{-1}(y/x))) \nonumber \\
&\quad + y\delta(\sin(\text{tan}^{-1}(y/x)))\cos(\tan^{-1}(y/x))(\frac{x}{x^2 + y^2})(\frac{1}{x})
\end{align}
Most of the time the Dirac terms $\delta(\cdot)$ will be zero and $\frac{\partial u}{\partial x} = \frac{\partial v}{\partial y} = \text{H}(\sin(\tan^{-1}(y/x)))$ as well as $\frac{\partial u}{\partial y} = - \frac{\partial v}{\partial x} = 0$ will hold. The derivative is zero if the non-linearity is inactive and 1 when its active and therefore bounded.


\subsection{Analysis of existing complex activation functions.}
This section is dedicated to the analysis of previously proposed non-linearities and relies on using the Cauchy-Riemann equations given by:
\begin{align}
\frac{\partial u}{\partial x} = \frac{\partial v}{\partial y}, \;\;\; \frac{\partial u}{\partial y} = - \frac{\partial v}{\partial x},
\end{align}
for $f(z) = u(x,y) + iv(x,y)$ or if $f(r, \theta) = u(r, \theta) + iv(r, \theta)$, we make use of:
\begin{align}
\frac{\partial u}{\partial r} = \frac{1}{r} \frac{\partial v}{\partial \theta} \; \text{and} \; \frac{\partial v}{\partial r} = - \frac{1}{r} \frac{\partial u}{\partial \theta}
\end{align}
which is equivalent.
\subsubsection{zRelu}
\begin{align}
\text{zRelu}(z) =\begin{cases} z \text{ if } \theta \in [0, \pi/2], \\
                               0 \text{  else}.
                 \end{cases}
\end{align}
Following \cite{Trabelsi} we have for the first quadrant:
\begin{align}
\frac{\partial u}{\partial x} = \frac{\partial v}{\partial y} = 1,\\
\frac{\partial u}{\partial y} = -\frac{\partial v}{\partial x} = 0, 
\end{align}
and elsewhere:
\begin{align}
\frac{\partial u}{\partial x} = \frac{\partial v}{\partial y} = 0,\\
\frac{\partial u}{\partial y} = -\frac{\partial v}{\partial x} = 0,
\end{align}
holds. On the real and imaginary axes, the two areas are not smoothly connected, which is why we must include them. This argument may be backed up by considering the equivalent Phase-Relu formulation as defined in section~\ref{sec:CarthesianPhaseRelu}:
\begin{align}
\text{zRelu}(z) = \text{H}(\sin(\text{atan2}(x,y)))\text{H}(\cos(\text{atan2}(x,y)))(x + iy)
\end{align}
Which is an equivalent way to write the zRelu, its Dirac pulse derivatives are one on the real and imaginary axis, which is why the this non-linearity is not holomorph there. The derivative is either zero or one and therefore bounded. These desirable properties come at the cost of having to throw away three quarters of the complex plane, which seems unnecessarily wasteful.

\subsubsection{modRelu}
\label{sec:modRelu}
The modRelu is defined as \cite{Arjovsky}:
\begin{align}
f(z) = \text{Relu}(\|z\| + b) \frac{z}{\|z\|}.
\end{align}
Conversion to polar coordinates yields:
\begin{align}
f(r, \theta) &= \text{Relu}(r + b)e^{i\theta}, \\
f(r, \theta) &= \text{Relu}(r + b)\cos(\theta) + i\text{Relu}(r + b)\sin(\theta). \\
\end{align}
we find $u(r, \theta) = \text{Relu}(r + b)\cos(\theta)$ and $v(r, \theta) = \text{Relu}(r + b)\sin(\theta)$. The polar Cauchy-Riemann equations yield:
\begin{align}
\frac{\partial u}{\partial r} &= \text{H}(r + b)\cos(\theta),  \\
\frac{\partial u}{\partial \theta} &= -\text{Relu}(r + b)\sin(\theta), \\
\frac{\partial v}{\partial r} &= \text{H}(r + b)\sin(\theta), \\
\frac{\partial v}{\partial \theta} &= \text{Relu}(r + b)\cos(\theta).
\end{align}
For holomorphy we require:
\begin{align}
\frac{\partial u}{\partial r} &= \frac{1}{r} \frac{\partial v}{\partial \theta}, \\
\Leftrightarrow r\text{H}(r + b)\cos(\theta) &= \text{Relu}(r + b)\cos(\theta);\\
\frac{\partial v}{\partial r} &= - \frac{1}{r} \frac{\partial u}{\partial \theta}, \\ 
\Leftrightarrow r\text{H}(r + b)\sin(\theta) &= \text{Relu}(r + b)\sin(\theta).
\end{align}
Taking into account the fact that $r\text{H}(r) = \text{Relu}(r)$ we have $r\text{H}(r + b) \approx \text{Relu}(r + b)$ if $b \approx 0$. The modRelu non-linearity is therefore only holomorph when it is approximately linear, not a useful property. Furthermore we find:
\begin{equation}
\frac{\partial}{\partial z}\sigma_{\text{Relu}}(\|z\| + b) \frac{z}{\|z\|} = \sigma_{\text{Relu}}'(\|z\| + b) \frac{z}{\|z\|} + \sigma_{\text{Relu}}(\|z\| + b) (\frac{z}{\|z\|})'.
\end{equation}
By applying the product rule. The left part of the resulting sum is stable, but the right part is not bounded and therefore unstable, which is yet another undesirable property.

\subsubsection{cRelu}
\cite{Trabelsi} defines the cRelu as:
\begin{align}
\text{cRelu}(z) = \text{Relu}(x) + i\cdot \text{Relu}(y).
\end{align}
Thus $u = \text{Relu}(x)$ and $v = \text{Relu}(y)$.
In the first quadrant we have:
\begin{align}
\frac{\partial u}{\partial x} = \frac{\partial v}{\partial y} = 1,\\
\frac{\partial u}{\partial y} = -\frac{\partial v}{\partial x} = 0.
\end{align}
For the second we find:
\begin{align}
\frac{\partial u}{\partial x} = 0 \neq \frac{\partial v}{\partial y} = 1,\\
\frac{\partial u}{\partial y} = -\frac{\partial v}{\partial x} = 0, 
\end{align}
The third quadrant has:
\begin{align}
\frac{\partial u}{\partial x} = \frac{\partial v}{\partial y} = 0,\\
\frac{\partial u}{\partial y} = -\frac{\partial v}{\partial x} = 0, 
\end{align}
Finally considering the fourth:
\begin{align}
\frac{\partial u}{\partial x} = 1 \neq \frac{\partial v}{\partial y} = 0,\\
\frac{\partial u}{\partial y} = -\frac{\partial v}{\partial x} = 0, 
\end{align}
In its holomorph region the derivative of the function is one just like in the real case which is stable.
We have shown this definition to be holomorph when $\text{sign}(\Re(z)) = \text{sign}(\Im(z))$\cite{Trabelsi}, which is the case in the first an third quadrant. When restricting this definition to the first quadrant and setting it to zero elsewhere, one obtains the $\text{zRelu}$ which is a holomorph function. \\

\subsubsection{Cardioid}
\cite{Virtue} introduces the complex cardioid,:
\begin{align}
f(z) = \frac{1}{2}(1 + cos(\theta))z
\end{align}
Which we express in polar-coordinates as:
\begin{align}
f(r, \theta) = \frac{1}{2}(1 + cos(\theta))re^{i\theta}
\end{align}
Using the definition of the complex exponential we obtain:
\begin{align}
f(r, \theta) = \frac{1}{2}(1 + cos(\theta))r\cos(\theta) + i\frac{1}{2}(1 + cos(\theta))r\sin(\theta) 
\end{align}
Reading of $u = \frac{1}{2}(1 + cos(\theta))r\cos(\theta)$ and $v = \frac{1}{2}(1 + cos(\theta))r\sin(\theta)$ we find:
\begin{align}
\frac{\partial u}{\partial r} &= \frac{1}{2}(1 + \cos(\theta))\cos(\theta) \\
\frac{\partial u}{\partial \theta} &= -r\frac{1}{2}(1 + \cos(\theta))\sin(\theta) - r\frac{1}{2}\sin(\theta)\cos(\theta) \\
\frac{\partial v}{\partial r} &= \frac{1}{2}(1 + \cos(\theta))\sin(\theta) \\
\frac{\partial v}{\partial \theta} &= r\frac{1}{2}(1 + \cos(\theta))\cos(\theta)) - r\frac{1}{2}\sin(\theta)\cos(\theta)
\end{align}
And therefore we require:
\begin{align}
\frac{\partial u}{\partial r} = \frac{1}{2}(1 + \cos(\theta))\cos(\theta) &= \frac{\partial v}{r\partial \theta} =  \frac{1}{2}(1 + \cos(\theta)\cos(\theta)) - r\frac{1}{2}\sin(\theta)\cos(\theta) \\
\Leftrightarrow 0  &= - r\frac{1}{2}\sin(\theta)\cos(\theta) \\
\end{align}
Which is holomorph at $r=0$. When $r \neq 0$ excluding everything except for the real and imaginary axes is necessary, there the trigonometric functions are zero. Where the derivative exists it is unstable when $\cos(\theta) > 0$, which happens if $0 \leq \theta < \pi/2$ and $3/2\pi < \theta \leq 2\pi$. This is the case on the real axis where $\theta = 0$. In other words the Cardioid has a defined and stable derivative only on the imaginary axis.

\subsubsection{Phase-amplitude activation}
\cite{Scardapane} cites these as:
\begin{align}
f(z) = \tanh(r/m)e^{i\theta}
\end{align}
Considering the polar C.R. equations:
\begin{align}
f(r,\theta) &= \cos(\theta)\tan^{-1}(r/m) + i\sin(\theta)\tan^{-1}(r/m) \\
            &\Rightarrow u + iv \nonumber \\
\frac{\partial u}{\partial r} &= \cos(\theta)\text{sech}^2(r/m)(1/m) \\
\frac{\partial u}{\partial \theta} &= -\sin(\theta)\tanh(r/m) \\
\frac{\partial v}{\partial r} &= \sin(\theta)\text{sech}^2(r/m)(1/m) \\
\frac{\partial v}{\partial \theta} &= \cos(\theta)\tanh(r/m) \\
\Rightarrow  \frac{\partial u}{\partial r} &=\frac{\partial v}{r\partial \theta} \nonumber \\
\Leftrightarrow \qquad & \cos(\theta)\text{sech}^2(r/m)(1/m) = \frac{1}{r}\cos(\theta)\tanh(r/m) \\
\Leftrightarrow \qquad & \text{sech}^2(r/m)(r/m) = \tanh(r/m)
\label{eq:holo-pa}
\end{align}
\begin{figure}
\centering
\includegraphics{./img/phase_amplitude_cond.pdf}
\caption{Plot of the holomorphy condition's two sides for the Phase-amplitude activation. $\tanh(r/m)$ is shown in red, $\text{sech}^2(r/m)(r/m)$ is shown in blue. The yellow circle indicates the point at $(0,0)$.}
\label{fig:phase_amp}
\end{figure}
This activation is holomorph at $r/m = 0$. And approximately analytic for $r/m \approx 0$ as shown in figure~\ref{fig:phase_amp}, the problem here is that this non-linearity breaks magnitude information by rescaling them, it must therefore be non-holomorph for large inputs. 

\subsection{Backward mode automatic differentiation gradients}
Consider the non-linear network proposed in \cite{Pascanu}:
\begin{align}
\mathbf{x}_t  = W_{\text{rec}}f(\mathbf{x}_{t-1}) + W_{\text{in}}\mathbf{u}_t + \textbf{b}
\end{align}
Following \cite{Pascanu} we define the error function $\mathcal{E}_t = \mathcal{L}$ and work with BPTT gradients given by:
\begin{align}
\frac{\partial \mathcal{E}}{\partial \theta} &= \sum_{1 \leq t \leq T} \frac{\mathcal{E}_t}{\partial \theta} \\
\frac{\partial\mathcal{E}_t}{\partial\theta} &= \sum_{1 \leq k \leq t}( \frac{\mathcal{E}_t}{\partial \mathbf{x}_t} \frac{\partial \mathbf{x}_t}{ \mathbf{x}_k} \frac{\partial^+ \mathbf{x}_k}{\partial \theta}) \label{eq:gradsum}\\
\frac{\partial \mathbf{x}_t}{ \partial \mathbf{x}_k} &= \prod_{t \geq i > k} \frac{\partial \mathbf{x}_i}{ \partial \mathbf{x}_{i-1}} = \prod_{t \geq i > k} W^T_{\text{rec}} \text{diag}(f'(\mathbf{x}_{i-1}))
\end{align}
The above equations are essential a consistent application of the chain rule. It is important to note that $\partial^+ \mathbf{x}_{k} / \partial W_\text{rec} = f(\mathbf{x}_{k-1})$.

\subsubsection{The linear case}
Working with $f(x) = x$ according the long term behavior is determined by the matrix product $\partial \mathbf{x}_t / \partial \mathbf{x}_k$ \cite{Pascanu}. We define $W_{\text{rec}} = C$, and normalize the spectrum of C to $\forall k \; \| \phi_k \| = 1$ for $k \in \{1 \dots n\}$. Focusing on the term in the sum of equation~\ref{eq:gradsum}, we express $\partial \mathcal{E} / \partial \mathbf{x}_t $ in terms of a Fourier basis:
\begin{align}
\frac{\partial \mathcal{E}}{\partial \mathbf{x}_t} = \sum_{i=1}^n \mathbf{f}_i^T d_i  
\end{align}
Knowing the Fourier vectors are eigenvectors of $C$, which leads to $\mathbf{f}_i^T(C^T)^l = \phi_i^l\mathbf{f}_i^T$ therefore we have:
\begin{align}
\frac{\partial \mathcal{E}}{\partial \mathbf{x}_t}\frac{\partial \mathbf{x}_t}{\partial \mathbf{x}_k} = 
\sum_{i=1}^n \mathbf{f}_i^T d_i \phi_i 
\end{align}
Having chosen $\| \phi_k \| = 1$, we can approximate:
\begin{align}
\frac{\partial \mathcal{E}}{\partial \mathbf{x}_t}\frac{\partial \mathbf{x}_t}{\partial \mathbf{x}_k} = 
\sum_{i=1}^n \mathbf{f}_i^T d_i \phi_i 
\approx \sum_{i=1}^n \mathbf{f}_i^T d_i =\frac{\partial \mathcal{E}}{\partial \mathbf{x}_t}
\end{align}
Because $\phi = \exp(i\omega)$ merely represents a rotation of the errors phase angle, but leaves its magnitude intact.
Using the train of tought borrowed from \cite{Pascanu}, we claim to establish an error carousel similar to Hochreiter 1998, through which errors can pass in a stable manner.

\subsubsection{Non-linear Cayley-Networks}
We could employ a non-linearity based on the Cayley-Transform \cite[p. 100]{Bornemann}:
\begin{equation}
C'(z) = \frac{z-i}{z+i}
\end{equation}
Which is guaranteed to map the upper half of the complex plane into the unit circle. Integrating $C(z)$
leads to;
\begin{equation}
C(z) = z - 2i\ln(z + i)
\end{equation}
Which leads to a possible non-linearity for $\Re(z) > 0$. The unstable lower part of the complex plane where $\Re(z) < 0$, could be removed by defining:
\begin{equation}
D'(z) = \frac{z+i}{z-i}
\end{equation}
And working with $D(z) = z + 2i\ln(z - i)$ where  $\Re(z) < 0$. Working with this definition all $z$ with $\Im(z) = 0$, would not be defined, because there is no smooth connection when crossing from $\Re(z) > 0$ to $\Re(z) < 0$ and the complex logarithm is not defined for all $\Re(z) < 0$ with $\Im(z) = 0$.
Cayley transforms are known to be holomorph, which is a general property of all MÃ¶bius transforms.


\subsection{Convolutions and circulant matrices}
One dimensional convolutions can be expressed as multiplication with a circulant matrix.
The convolution operations used in neural networks may be expressed as matrix multiplication
with doubly circulant matrices \cite[page 324]{goodfellow}, doubly referring to a circulant block matrix consisting of circulant blocks. 
The eigen-decompositions of both cases seem to be well understood in the specialized mathematical literature\footnote{\url{http://nzjm.math.auckland.ac.nz/images/8/8e/18-36.pdf}}.

\subsection{1D-convolutions and circulant matrices}
\begin{figure}
\centering
\begin{minipage}{.45\textwidth}
\begin{equation*}
C = \begin{pmatrix}
c_1 & c_2 & c_3 & c_4 \\
c_4 & c_1 & c_2 & c_3 \\
c_3 & c_4 & c_1 & c_2 \\
c_2 & c_3 & c_4 & c_1 \\
\end{pmatrix}
\end{equation*}
\end{minipage}
\begin{minipage}{.45\textwidth}
\scalebox{0.4}{\input{img/circ.tex}}
\end{minipage}
\caption{Circulant matrix structure as formula and in plotted form.}
\label{fig:circ1}
\end{figure}
Consider for example the four by four circulant matrix $C = circ(c_1,c_2,c_3,c_4)$
as shown in figure~\ref{fig:circ1}.
The matrix vector product $C\mathbf{x}$ with $\mathbf{x} = (x_1, x_2, x_3, x_4)^T$, can be written as:
\begin{align}
c_1x_1 + c_2x_2 + c_3x_3 + c_4x_4 \\
c_4x_1 + c_1x_2 + c_2x_3 + c_3x_4 \\
c_3x_1 + c_4x_2 + c_1x_3 + c_2x_4 \\
c_2x_1 + c_3x_2 + c_4x_3 + c_1x_4
\end{align}
Above one can nicely see how the kernel moves over the one dimensional signal in x. If the wrapping effect is not desired the edges of $x$ must be padded with zeros and parts of the circulant matrix be set to zero. For example $x_1, x_4 =0$ and $c_3,c_4=0$, will remove the wrap-around.

\subsection{The linear one-dimensional case}
According to \cite[page 33]{gray}, the eigenbasis of all circulant matrices is given by:
\begin{align}
\mathbf{f}^{(m)} = \dfrac{1}{\sqrt{n}}(1, \exp(-2\pi im/n), \dots, \exp(-2\pi im(n-1)/n))'
\end{align}
For all m eigenvectors. Given a set of complex eigenvalues $\{\phi_m\}$, the corresponding
circulant matrix can be computed using:
\begin{align}
C = F \Phi F^{-1}
\end{align}
For reasons which will become clear later we will express our eigenvalues in polar coordinates
as:
\begin{align}
\phi_m = r \exp{i\omega}
\end{align}
We propose the normalize network convolutions by setting their $r=1$ for all convolutions and all eigenvalues and optimize only the eigenangles $\omega$. Which amounts to requiring all
convolution matrices to have eigenvalues located on the unit circle or equivalently, we enforce 
$\| \phi_m \| = 1$. To construct $C$ we must transform all $\phi_m$s to Cartesian coordinates 
using $x_m= \cos(\omega)$ and $y_m=\sin(\omega)$. When then place the Cartesian eigenvalues $x_m + iy_m$ on the diagonal of $\Phi$. Next we can construct $C$ from $C = U \Phi U^{-1}$. Where $U$ is known and can be attached as a constant matrix to the computational graph. Furthermore the previous operations involve only trigonometric functions and matrix products, these operations are all differentiable, therefore we can find their gradient using standard AD tools.
By running the optimization in the $\omega$ space we can enforce $\| \phi_m \| = 1$, without having to work with a constrained optimization algorithm.

\subsection{Effects on linear network stability}
\subsubsection{Matrix power}   
\label{seq:linstab}
In this section we will evaluate the effect of $\| \phi_m \| = 1$ on a linear one dimensional bias-free convnet consisting of n layers.
\begin{align}
y &= C_1 \cdot C_2 \dots C_n \cdot x \\
y &= F \Phi_1 F^{-1} \cdot F\Phi_2 F^{-1} \dots \cdot F\Phi_n F^{-1} \cdot x \\
y &= F \Phi_1 \cdot \Phi_2 \dots \cdot \Phi_n F^{-1}\cdot x \\ 
\end{align}
All convolution eigenvalues will be of the form $\phi_{m,n} = \exp{i\omega_{m,n}}$, $\Phi$ amounts to element wise multiplication considering the rows therefore will lead to eigenvalues of:
\begin{align}
\phi_m = \exp(i\omega_{m,1} + i\omega_{m,2} \dots i\omega_{m,n} )
\end{align}
\begin{figure}
    \centering
    \begin{tikzpicture}
        \draw[thick] (0cm, 0cm) circle(1cm);
        \draw[->] (0,-1.25cm) -- (0,1.25cm) node[above] {$y$};
        \draw[->] (-1.25,0) -- (1.25cm,0) node[right] {$x$};
        \filldraw[black] (1cm, 0) circle(1.4pt);
        \draw (1.35cm,0cm) node[above=1pt] {$(1,0)$};
    \end{tikzpicture}
    \caption{Illustration of the unit circle, on which we place all convolution
    eigenvalues $\phi = x + iy$.}
\end{figure}
For the equivalent one convolution network. We therefore claim that adding convolutions to this kind of eigenspace normalized linear network will add additional degrees of freedom to eigenspace rotations around the unit circle. Having set all $r_{m,n} = 1$ we claim to run a more stable network, because we only rotate, but do not rescale with additional layers.
This convolution should remain stable when added to the  recurrent convLSTM state update equation. \\
However to apply this idea to convNets in space the non-linearity needs to be taken care of.

\subsubsection{Network conditioning}
In linear algebra when solving $A\mathbf{x} = b$ or $\min_x \| Ax - b\|$ an important property
is the condition number. It is a measure of the solutions sensitivity to small perturbations in $\mathbf{x}$. A problem is considered to be ill conditioned when $A$'s assiciated condition number is very large. 
A problem's conditioning is measured using:
\begin{align}
\kappa = \text{max}_{i,j}| \dfrac{\phi_i}{\phi_j}|
\end{align}
In other words the matrix condition $\kappa$ is the norm of the ratio of the largest and smallest eigenvalue. By enforcing $\| \phi \| = 1$ we also ensure a constant condition number of one for our convolution matrices. We hope to increase overall network stability this way, because our convolutions should not react very sensitively to small input perturbations.

\subsection{Two dimensional convolutions}
Discrete convolution is often described as sliding a kernel over an image. This operation may be expressed in terms of matrix-vector multiplication. For example the two dimensional convolution:
\begin{align}
A * B =
\begin{pmatrix}
a_1 & a_2 \\
a_3 & a_4
\end{pmatrix}
*
\begin{pmatrix}
b_1 & b_2 \\
b_3 & b_4
\end{pmatrix}
\end{align}
May be expressed using matrix multiplication as:
\begin{align}
A*B = K^T \cdot B_{\text{flat}}
\end{align}
Where $b_{\text{flat}}$ is a vector constructed by concatenation of B's rows.
And the matrix K defined as:
\begin{align} K =
\begin{pmatrix}
a_1 & a_2 & 0 & a_3 & a_4 & 0   & 0 & 0 &   0 \\
0   & a_1 & a_2 & 0 & a_3 & a_4 & 0 & 0 &   0 \\
0   & 0 & 0 & a_1 & a_2 & 0 & a_3 & a_4 &   0 \\
0   & 0 & 0 & 0   & a_1 & a_2 & 0 & a_3 & a_4 \\
\end{pmatrix}
\end{align}
Matrix $K$, describes a convolution, but is not circulant. 

\subsubsection{Doubly block circulant matrices}
\begin{figure}
\input{img/Kt.tex}
\input{img/2dcirc.tex}
\caption{Visualization of a two dimensional convolution matrix and its square doubly circulant cousin.}
\label{fig:2dconv}
\end{figure}
In order to turn the convolution matrix into a square doubly circulant matrix, padding is required in both kernel and target matrix. A doubly circulant matrix is a block matrix consisting out of circulant blocks which are arranged in a circular pattern. In order to obtain circulant blocks the circular pattern must be finished, which is why the resulting matrix will be square by definition.
Padding $A$ and $B$ leads to\footnote{I think its probably possible to come up with a less wasteful way to do the padding i.e. remove the second zero row and column.}:
\begin{align}
A_p = \begin{pmatrix}
a_1 & a_2 & 0 & 0 \\
a_3 & a_4 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
\end{pmatrix}
B_p = \begin{pmatrix}
b_1 & b_2 & 0 & 0 \\
b_3 & b_4 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
\end{pmatrix}
\end{align}
In this case the circular convolution matrix can be set up according to:
\begin{align}
C_0 &= circ(c_0) = \begin{pmatrix} a_1 & a_2 & 0 & 0 \end{pmatrix} \\
C_1 &= circ(c_1) = \begin{pmatrix} a_2 & a_3 & 0 & 0 \end{pmatrix} \\
C_2 &= circ(c_2) = \begin{pmatrix} 0 & 0 & 0 & 0 \end{pmatrix} \\
C_3 &= circ(c_3) = \begin{pmatrix} 0 & 0 & 0 & 0 \end{pmatrix} \\
\end{align}
Which leads to the resulting matrix $C$:
\begin{align}
C_b = \begin{pmatrix} C_0 & C_1 & C_2 & C_3 \\
                    C_3 & C_0 & C_1 & C_2 \\
                    C_2 & C_3 & C_0 & C_2 \\
                    C_1 & C_2 & C_3 & C_0 \\
    \end{pmatrix}
\end{align}
A visualization of this matrix is shown in figure~\ref{fig:2dconv} on the right. Multiplication of $C_b \cdot B_{p\text{ flat}}$ will lead to a zero padded version of $K^T \cdot B_{\text{flat}}$.


\subsubsection{Doubly block circulant matrices and their eigenvalues}
\begin{figure}
\centering
\input{img/2dcircfreq.tex}
\caption{Absolute values of complex doubly block circulant matrix constructed in the frequency domain.}
\end{figure}

In order to be able to enforce $\| \phi \| = 1$. We would like to be able to construct doubly block circulant matrices in their eigenspace. According to \cite[page 185]{Davis}, their diagonalization is given by:
\begin{align} 
C_b = \overline{(F_m \otimes F_n)^T} \Lambda (F_m \otimes F_n)
\label{eq:dc_spec}
\end{align}
$F_m$ and $F_n$ denote fourier matrices. $\Lambda$ has complex eigenvalues sitting on its diagonal. Their choice determines the block structure of the resulting 
matrix, which will be square with $m$ block containing $n$ rows each.
Given a real input matrix we can find Lambda from:
\begin{align}
\Lambda = (F_m \otimes F_n) C_b \overline{(F_m \otimes F_n)^T}
\end{align}
We believe that~\ref{eq:dc_spec} is differentiable and should enable use to construct and optimize doubly block circulant matrices in the frequency domain. In order to ensure a real valued output $\Lambda$ must be symmetric with respect to the real axis.

\subsubsection{The spectrum of real doubly block circulant matrices.}
Tricky because doubly block circulants are not also circulant. So we cannot simply apply the one dimensional insight gained from working with circulants to block circulants. However block circulant spectra are point-symmetric in tow dimensions. Figures~\ref{fig:circ_spec2d} and~\ref{fig:block_circ_spec2d} illustrate this. The spectrum shown in ~\ref{fig:circ_spec2d} is symmetric along the a-Axis, when cutting after its third element and disregarding the first eigen-vale. The block circulant case shown in ~\ref{fig:circ_spec2d}, we find the same symmetry in the zeroth column and 4th row. The middle block is point symmetric. 

\begin{figure}
\centering
\input{img/circ_symmetrie/circ.tex}
% \input{img/denseCircEigs.tex}
\caption{Circulant matrix pattern and spectrum.}
\label{fig:circ_spec2d}
\end{figure} 

\begin{figure}
% \input{img/circ16.tex}
\input{img/circ_symmetrie/2dcirc.tex}
\caption{Block Circulant matrix pattern and spectrum.}
\label{fig:block_circ_spec2d}
\end{figure}

\section{Other related ideas}
\subsection{Rotation-GRU in $\mathbb{R}$}
This section proposes the rotation-GRU, a modified version of the conv-GRU, which builds on the theory above. Recall the conv-GRU definition:
\begin{align}
Z_t &= \sigma(W_{xz} * X_t + W_{hz} * H_{t-1} + b_z), \\
R_t &= \sigma(W_{xr} * X_t + W_{hr} * H_{t-1} + b_r), \\
H_t' &= f(W_{xr} * X_t) + R_t \circ (W_{hp} * H_{t-1}), \\
H_t &= (1 - Z_t) \circ H_t' + Z_t \circ H_{t-1}.
\end{align}
When optimizing the convolutions, while enforcing $\| \phi \| = 1$, 
changing the state update equation $H_t$ to:
\begin{align}
H_t &= W_h * ((1 - Z_t) \circ H_t' + Z_t \circ H_{t-1}).
\end{align}
Assuming that the gates $Z_t$ and $R_t$ keep the absolute value of 
$((1 - Z_t) \circ H_t' + Z_t \circ H_{t-1})$ under control, like they learn to
do in the standard conv-GRU case, the network should remain stable, because the eigenvalues of $W_h$ are normalized. A rational similar to the one in section~\ref{seq:linstab} should hold.

\subsubsection{Gradients of the Rotation-GRU}
So far we have only considered the forward pass of the optimization process. In order for our ideas to work we must also consider the backward pass. The two are similar, because in time, input and error flow follow the dynamics of the state equation $H_t$. This section examines the gradient equations for the convGRU and rotationGRU in detail. \dots ..TODO!

\bibliographystyle{plain}
\bibliography{literature}
% \begin{thebibliography}{9}
% \bibitem[Goodfellow]{goodfellow} \emph{Deep Learning},
% MIT Press 2017
% \bibitem[Strang]{strang} \emph{Linear algebra},
% MIT Press 2006
% \bibitem[Gray]{gray} \emph{Toeplitz and Circulant Matrices: A Review},
% now publishing
% \bibitem[Bronstein]{Bronstein} \emph{Springer Taschenbuch der Mathematik}, Springer Spektrum
% % \bibitem[Gers et al]{Gers} \emph{Learning to Forget: Continual Prediction with LSTM}
% \bibitem[Davis]{Davis} \emph{Circulant Matrices}, John Wiley and Sons
% \bibitem[Pascanu]{Pascanu} \emph{On the difficulty of training Recurrent Neural networks}, \url{https://arxiv.org/pdf/1211.5063.pdf}
% \bibitem[Arjovsky]{Arjovsky} \emph{Unitary Evolution Recurrent Neural networks.}
% \bibitem[Briggs]{Briggs} \emph{The DFT, an Owners Manual for the Discrete Fourier Transform.}
% \bibitem[Bornemann]{Bornemann} \emph{Funktionentheorie}, \url{http://www.springer.com/de/book/9783034804721}
% \bibitem[Trabelsi]{Trabelsi} \emph{Deep Complex Networks}, ICLR 2018 \url{https://arxiv.org/pdf/1705.09792.pdf}
% \bibitem[Hyland]{Hyland}, \emph{Learning Unitary Operators with Help From u (n).}, aaai 2017, \url{http://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14930/14373}
% \bibitem[Guberman]{Guberman} \emph{On Complex Valued Convolutional Neural Networks}, \url{https://arxiv.org/pdf/1602.09046.pdf}
% \bibitem[Wisdom]{Wisdom} \emph{Full-Capacity Unitary Recurrent Neural Networks}, \url{https://arxiv.org/abs/1611.00035}
% \bibitem[Jing]{Jing} \emph{Gated Orthogonal Recurrent Units: On Learning to Forget}, \url{https://arxiv.org/pdf/1706.02761.pdf}
% \bibitem[Trabelsi]{Trabelsi} \emph{Deep Complex Networks}, \url{https://arxiv.org/pdf/1705.09792.pdf}
% \bibitem[Freeman]{Freeman} \emph{The Design and Use of Steerable Filters}, \url{http://people.csail.mit.edu/billf/www/papers/steerpaper91FreemanAdelson.pdf}
% \bibitem[Michaelis]{Michaelis} \emph{A lie group approach to steerable filters}, \url{https://www.sciencedirect.com/science/article/pii/016786559500066P?via%3Dihub}
% \bibitem[Virtue]{Virtue} \emph{BETTER THAN REAL: COMPLEX-VALUED NEURAL NETS FOR MRI FINGERPRINTING}, \url{https://arxiv.org/pdf/1707.00070.pdf}
% \bibitem[Scardapane]{Scardapane}, \emph{Complex-valued Neural Networks with Non-parametric Activation Functions}, \url{https://arxiv.org/pdf/1802.08026.pdf}
% \end{thebibliography}

\end{document}